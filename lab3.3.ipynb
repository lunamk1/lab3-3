{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07d1799c-84ba-4bf8-86ef-4abd9d9c2424",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3859fffc-5883-438a-b64f-dea9c9dbfbf1",
   "metadata": {},
   "source": [
    "### Ridge Regression\n",
    "In this step, we use a pre-trained BERT model (bert-base-uncased) to extract CLS token embeddings for each TR (timepoint) in every story. Each TR contains a list of words, which are tokenized and encoded using BERT. These embeddings are then aligned with their corresponding fMRI responses. We apply z-score normalization and temporal delays to the BERT embeddings and fit a ridge regression model to predict voxel-level brain activity from the BERT representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc0fc577-d2d8-4c78-849f-bc1b6f2a0f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw_text.pkl and fMRI...\n",
      "Valid stories with fMRI: 101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing stories: 100%|██████████| 101/101 [03:18<00:00,  1.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collected 34700 TRs | BERT dim: 768 | Voxels: 94251\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ridge_utils.utils import zscore, make_delayed\n",
    "from ridge_utils.ridge import ridge_corr\n",
    "\n",
    "# -------------------- Step 1: Loading -------------------- #\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "data_dir = \"/ocean/projects/mth240012p/shared/data\"\n",
    "subject = \"subject2\"\n",
    "max_tokens = 50\n",
    "\n",
    "print(\"Loading raw_text.pkl and fMRI...\")\n",
    "with open(os.path.join(data_dir, \"raw_text.pkl\"), \"rb\") as f:\n",
    "    raw_texts = pickle.load(f)\n",
    "\n",
    "story_names = []\n",
    "Y_dict = {}\n",
    "for story in raw_texts:\n",
    "    fmri_path = os.path.join(data_dir, subject, f\"{story}.npy\")\n",
    "    if os.path.exists(fmri_path):\n",
    "        Y_dict[story] = np.load(fmri_path)\n",
    "        story_names.append(story)\n",
    "\n",
    "print(f\"Valid stories with fMRI: {len(story_names)}\")\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\") # loading BERT Model\n",
    "bert_model = BertModel.from_pretrained(\"bert-base-uncased\").to(device)\n",
    "bert_model.eval()\n",
    "\n",
    "# -------------------- Step 2: Iterate over each story and each TR -------------------- #\n",
    "X_all, Y_all = [], []\n",
    "\n",
    "for story in tqdm(story_names, desc=\"Processing stories\"):\n",
    "    fmri = Y_dict[story]\n",
    "    ds = raw_texts[story] \n",
    "    for i in range(len(ds.data)):\n",
    "        word_list = ds.data[i] \n",
    "        if not word_list or i >= fmri.shape[0]:\n",
    "            continue  \n",
    "        sentence = \" \".join(word_list)\n",
    "        inputs = tokenizer(\n",
    "            sentence,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"max_length\",\n",
    "            max_length=512,\n",
    "            truncation=True\n",
    "        )\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = bert_model(**inputs)\n",
    "        cls_embed = outputs.last_hidden_state[:, 0, :].squeeze().cpu().numpy()\n",
    "        X_all.append(cls_embed)\n",
    "        Y_all.append(fmri[i])\n",
    "\n",
    "X_all = np.array(X_all)\n",
    "Y_all = np.array(Y_all)\n",
    "\n",
    "print(f\"\\nCollected {X_all.shape[0]} TRs | BERT dim: {X_all.shape[1]} | Voxels: {Y_all.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "186a78b1-142a-4176-9616-a678fc9698c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running ridge regression...\n",
      "Final shapes → X: (34696, 3840), Y: (34696, 94251)\n",
      "Train: (27756, 3840), Test: (6940, 3840)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_train\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Test: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_test\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m alphas \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlogspace(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m20\u001b[39m)\n\u001b[0;32m---> 19\u001b[0m ccs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(ridge_corr(X_train, X_test, Y_train, Y_test, alphas))\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRidge correlation shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, ccs\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     22\u001b[0m best_cc \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(ccs, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/lab3-3/code/../ridge_utils/ridge.py:269\u001b[0m, in \u001b[0;36mridge_corr\u001b[0;34m(Rstim, Pstim, Rresp, Presp, alphas, normalpha, corrmin, singcutoff, use_corr, logger)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;66;03m# pred = np.dot(mult_diag(D, np.dot(Pstim, Vh.T), left=False), UR) ## Better (2.0 seconds to prediction in test)\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m# pvhd = reduce(np.dot, [Pstim, Vh.T, D]) ## Pretty good (2.4 seconds to prediction in test)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;66;03m# wt = reduce(np.dot, [Vh.T, D, U.T, Rresp]).astype(dtype) ## Worst\u001b[39;00m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# pred = np.dot(Pstim, wt) ## Predict test responses\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_corr:\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;66;03m#prednorms = np.apply_along_axis(np.linalg.norm, 0, pred) ## Compute predicted test response norms\u001b[39;00m\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;66;03m#Rcorr = np.array([np.corrcoef(Presp[:,ii], pred[:,ii].ravel())[0,1] for ii in range(Presp.shape[1])]) ## Slowly compute correlations\u001b[39;00m\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;66;03m#Rcorr = np.array(np.sum(np.multiply(Presp, pred), 0)).squeeze()/(prednorms*Prespnorms) ## Efficiently compute correlations\u001b[39;00m\n\u001b[0;32m--> 269\u001b[0m     Rcorr \u001b[38;5;241m=\u001b[39m (zPresp \u001b[38;5;241m*\u001b[39m zs(pred))\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;66;03m## Compute variance explained\u001b[39;00m\n\u001b[1;32m    272\u001b[0m     resvar \u001b[38;5;241m=\u001b[39m (Presp \u001b[38;5;241m-\u001b[39m pred)\u001b[38;5;241m.\u001b[39mvar(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/lab3-3/code/../ridge_utils/ridge.py:8\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(v)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mitertools\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mitools\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m zs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m v: (v\u001b[38;5;241m-\u001b[39mv\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m0\u001b[39m))\u001b[38;5;241m/\u001b[39mv\u001b[38;5;241m.\u001b[39mstd(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;66;03m## z-score function\u001b[39;00m\n\u001b[1;32m     10\u001b[0m ridge_logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mridge_corr\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mridge\u001b[39m(stim, resp, alpha, singcutoff\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-10\u001b[39m, normalpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, logger\u001b[38;5;241m=\u001b[39mridge_logger):\n",
      "File \u001b[0;32m/opt/packages/anaconda3-2024.10-1/lib/python3.12/site-packages/numpy/core/_methods.py:206\u001b[0m, in \u001b[0;36m_std\u001b[0;34m(a, axis, dtype, out, ddof, keepdims, where)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_std\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, ddof\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m    205\u001b[0m          where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 206\u001b[0m     ret \u001b[38;5;241m=\u001b[39m _var(a, axis\u001b[38;5;241m=\u001b[39maxis, dtype\u001b[38;5;241m=\u001b[39mdtype, out\u001b[38;5;241m=\u001b[39mout, ddof\u001b[38;5;241m=\u001b[39mddof,\n\u001b[1;32m    207\u001b[0m                keepdims\u001b[38;5;241m=\u001b[39mkeepdims, where\u001b[38;5;241m=\u001b[39mwhere)\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, mu\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m    210\u001b[0m         ret \u001b[38;5;241m=\u001b[39m um\u001b[38;5;241m.\u001b[39msqrt(ret, out\u001b[38;5;241m=\u001b[39mret)\n",
      "File \u001b[0;32m/opt/packages/anaconda3-2024.10-1/lib/python3.12/site-packages/numpy/core/_methods.py:173\u001b[0m, in \u001b[0;36m_var\u001b[0;34m(a, axis, dtype, out, ddof, keepdims, where)\u001b[0m\n\u001b[1;32m    168\u001b[0m     arrmean \u001b[38;5;241m=\u001b[39m arrmean \u001b[38;5;241m/\u001b[39m rcount\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# Compute sum of squared deviations from mean\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# Note that x may not be inexact and that we need it to be an array,\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# not a scalar.\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m x \u001b[38;5;241m=\u001b[39m asanyarray(arr \u001b[38;5;241m-\u001b[39m arrmean)\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(arr\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, (nt\u001b[38;5;241m.\u001b[39mfloating, nt\u001b[38;5;241m.\u001b[39minteger)):\n\u001b[1;32m    176\u001b[0m     x \u001b[38;5;241m=\u001b[39m um\u001b[38;5;241m.\u001b[39mmultiply(x, x, out\u001b[38;5;241m=\u001b[39mx)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X_z = zscore(X_all.T).T\n",
    "Y_z = zscore(Y_all)\n",
    "Y_z = np.nan_to_num(Y_z)\n",
    "\n",
    "X_delayed = make_delayed(X_z[4:], delays=[0, 1, 2, 3, 4])\n",
    "Y_trimmed = Y_z[4:]\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X_delayed, Y_trimmed, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# -------------------- Step 3: Ridge Regression -------------------- #\n",
    "print(\"Running ridge regression...\")\n",
    "\n",
    "print(f\"Final shapes → X: {X_delayed.shape}, Y: {Y_trimmed.shape}\")\n",
    "print(f\"Train: {X_train.shape}, Test: {X_test.shape}\")\n",
    "\n",
    "alphas = np.logspace(1, 3, 20)\n",
    "ccs = np.array(ridge_corr(X_train, X_test, Y_train, Y_test, alphas))\n",
    "\n",
    "print(\"Ridge correlation shape:\", ccs.shape)\n",
    "best_cc = np.max(ccs, axis=0)\n",
    "print(f\"Mean CC:    {np.mean(best_cc):.4f}\")\n",
    "print(f\"Median CC:  {np.median(best_cc):.4f}\")\n",
    "print(f\"Top 1% CC:  {np.mean(np.sort(best_cc)[-int(0.01*len(best_cc)):]):.4f}\")\n",
    "print(f\"Top 5% CC:  {np.mean(np.sort(best_cc)[-int(0.05*len(best_cc)):]):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73440276-29d6-4253-90f1-eed66afd4d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the Result \n",
    "print(\"X_delayed shape:\", X_delayed.shape)\n",
    "print(\"Y_trimmed shape:\", Y_trimmed.shape)\n",
    "print(\"NaN in X:\", np.isnan(X_delayed).any(), \"| Inf in X:\", np.isinf(X_delayed).any())\n",
    "print(\"NaN in Y:\", np.isnan(Y_trimmed).any(), \"| Inf in Y:\", np.isinf(Y_trimmed).any())\n",
    "print(\"X mean/std:\", np.mean(X_delayed), np.std(X_delayed))\n",
    "print(\"Y mean/std:\", np.mean(Y_trimmed), np.std(Y_trimmed))\n",
    "print(\"All zeros in Y:\", np.all(Y_trimmed == 0))\n",
    "nan_voxels = np.isnan(Y_all).sum(axis=0)\n",
    "print(f\"Max NaN count per voxel: {nan_voxels.max()} / {Y_all.shape[0]}\")\n",
    "print(f\"Number of voxels with any NaN: {np.sum(nan_voxels > 0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5010a9-40e4-4f29-8c34-eecca0814aef",
   "metadata": {},
   "source": [
    "### Fine-Tuning BERT Encoder on Stimulus Texts\n",
    "\n",
    "In this section, we fine-tune the BERT encoder using the raw story text data via a Masked Language Modeling (MLM) objective.\n",
    "\n",
    "We use the following approach:\n",
    "\n",
    "- **Dataset Construction**: We convert each story's TR-aligned text (from the `DataSequence`) into input IDs for BERT, masking a subset of tokens randomly to form an MLM task. Each TR is treated as one training example.\n",
    "- **Model Architecture**: We reuse the `Encoder` defined in `encoder.py`, which wraps a pre-trained BERT model (`bert-base-uncased`) with a linear decoder head to predict masked tokens.\n",
    "- **Training Loop**: We leverage `train_encoder.py` to fine-tune the model over multiple epochs. The training loss is computed only on masked positions.\n",
    "\n",
    "This fine-tuning step helps the BERT encoder adapt to the linguistic distribution of the experimental stimuli, improving the quality of the learned embeddings for downstream voxel prediction.\n",
    "\n",
    "After training, we will save the encoder weights, which can later be loaded and used for voxel-wise ridge regression in Part 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c886e40-a09e-41b3-b700-1346563f108b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================= Fine-tuning Setup ============================\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from train_encoder import train_bert\n",
    "from encoder import Encoder\n",
    "\n",
    "# 1. Prepare Dataset for MLM pretraining\n",
    "class MLMTextDataset(Dataset):\n",
    "    def __init__(self, raw_texts, tokenizer, max_length=128):\n",
    "        self.samples = []\n",
    "        for story in raw_texts.values():\n",
    "            for word_list in story.data:\n",
    "                if not word_list:\n",
    "                    continue\n",
    "                sentence = \" \".join(word_list)\n",
    "                tokens = tokenizer(\n",
    "                    sentence,\n",
    "                    max_length=max_length,\n",
    "                    truncation=True,\n",
    "                    padding='max_length',\n",
    "                    return_tensors='pt'\n",
    "                )\n",
    "                self.samples.append(tokens)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.samples[idx]\n",
    "        return {k: v.squeeze(0) for k, v in item.items()}\n",
    "\n",
    "# 2. Initialize model, tokenizer, and dataset\n",
    "print(\"Preparing dataset for fine-tuning...\")\n",
    "dataset = MLMTextDataset(raw_texts, tokenizer, max_length=128)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "print(\"Initializing encoder model...\")\n",
    "encoder = Encoder(vocab_size=tokenizer.vocab_size).to(device)\n",
    "\n",
    "# 3. Train the encoder using MLM objective\n",
    "print(\"Starting MLM training...\")\n",
    "train_bert(\n",
    "    model=encoder,\n",
    "    dataloader=dataloader,\n",
    "    tokenizer=tokenizer,\n",
    "    device=device,\n",
    "    epochs=3, # Change this if you want\n",
    "    lr=1e-4 # Can change into smaller\n",
    ")\n",
    "\n",
    "# 4. Save encoder for later use\n",
    "torch.save(encoder.state_dict(), \"finetuned_encoder.pt\") # use this model for new BERT Model\n",
    "print(\"Fine-tuned encoder saved as 'finetuned_encoder.pt'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6186a5-a8b4-4a5c-acda-7aea3de2dbdb",
   "metadata": {},
   "source": [
    "## Step 3 Fine-tuning with LoRa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee01428b-f1b5-40c8-a84d-bf81b60ad08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, BertModel, BertPreTrainedModel, TrainingArguments, Trainer\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from datasets import Dataset\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e568f5a6-34fb-4b85-9825-1a0e038fdfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a voxel to predict\n",
    "voxel_index = 0  # change this to try other voxels\n",
    "\n",
    "# Input text and corresponding voxel response\n",
    "X_train_text = X_train  # list of story segments (strings)\n",
    "X_test_text = X_test\n",
    "y_train_voxel = Y_train[:, voxel_index]\n",
    "y_test_voxel = Y_test[:, voxel_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab1ba12a-6fcc-4530-a0cd-a8ed5dde168d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "03d74195-1d1b-4872-b103-fc9e1bb6291a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae713d16bb4b43b7840f4f3087f1ab2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f1f264482154dab985800a9aad6ca38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "voxel_index = 0\n",
    "X_train_text = X_train\n",
    "X_test_text = X_test\n",
    "y_train_voxel = Y_train[:, voxel_index]\n",
    "y_test_voxel = Y_test[:, voxel_index]\n",
    "\n",
    "# Select subset first\n",
    "# train_dataset = Dataset.from_dict({\"text\": X_train_text, \"label\": y_train_voxel.tolist()}).select(range(200))\n",
    "# test_dataset = Dataset.from_dict({\"text\": X_test_text, \"label\": y_test_voxel.tolist()}).select(range(50)) \n",
    "\n",
    "\n",
    "def preprocess(example):\n",
    "    tokens = tokenizer(str(example[\"text\"]), truncation=True, padding=\"max_length\", max_length=128)\n",
    "    tokens[\"label\"] = example[\"label\"]\n",
    "    return tokens\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess)\n",
    "test_dataset = test_dataset.map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ba8c8ccb-ec84-497a-b43c-b7a17245c440",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertRegression were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['regressor.bias', 'regressor.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# --- Imports ---\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoConfig, BertModel, BertPreTrainedModel,\n",
    "    TrainingArguments, Trainer\n",
    ")\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "# --- Model Definition with Correct Output ---\n",
    "class BertRegression(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.regressor = nn.Linear(config.hidden_size, 1)\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, labels=None, **kwargs):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = self.dropout(outputs.pooler_output)\n",
    "        logits = self.regressor(pooled_output).squeeze(-1)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = nn.MSELoss()(logits, labels)\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "\n",
    "# --- Tokenizer & Config ---\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "config = AutoConfig.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# --- Model + LoRA ---\n",
    "model = BertRegression.from_pretrained(\"bert-base-uncased\", config=config)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"query\", \"value\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.FEATURE_EXTRACTION\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config).to(\"cuda\")\n",
    "\n",
    "# --- TrainingArguments (minimal) ---\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./lora-regression\",\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-5,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ae3eab6a-98a6-4944-931f-3ccc071370bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "No label_names provided for model class `PeftModelForFeatureExtraction`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='75' max='75' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [75/75 00:01, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.631400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.970000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.579800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.175800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.845000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.764800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.604200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.641400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.118200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.795700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.941000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.573200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>1.068300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.009800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.692800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=75, training_loss=1.0274275652567546, metrics={'train_runtime': 1.6011, 'train_samples_per_second': 374.734, 'train_steps_per_second': 46.842, 'total_flos': 39602199398400.0, 'train_loss': 1.0274275652567546, 'epoch': 3.0})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6cebc180-05e6-4d69-bf65-b4961ba6001c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"lora_finetuned_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2e100064-2816-4bce-b198-e90b8ec53bc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA R² for voxel 0: 0.0848\n"
     ]
    }
   ],
   "source": [
    "# Proper unpacking\n",
    "raw_preds = trainer.predict(test_dataset).predictions\n",
    "preds = raw_preds[0] if isinstance(raw_preds, tuple) else raw_preds\n",
    "\n",
    "# Flatten if needed\n",
    "preds = preds.flatten()\n",
    "\n",
    "# Compute R²\n",
    "r2 = np.corrcoef(preds, y_test_voxel[:len(preds)])[0, 1] ** 2\n",
    "print(f\"LoRA R² for voxel {voxel_index}: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7d28ca-1522-4652-b5da-066d17ea9bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import r2_score\n",
    "import torch\n",
    "\n",
    "def get_lora_bert_embeddings(sentences, model, tokenizer, batch_size=32):\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(sentences), batch_size):\n",
    "            batch = sentences[i:i+batch_size]\n",
    "            inputs = tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\", max_length=128).to(\"cuda\")\n",
    "            outputs = model.bert(**inputs)\n",
    "            pooled = outputs.pooler_output.cpu().numpy()\n",
    "            embeddings.append(pooled)\n",
    "\n",
    "    return np.vstack(embeddings)  # shape: (n_samples, hidden_size)\n",
    "\n",
    "# 1. Extract LoRA fine-tuned embeddings\n",
    "X_train_emb = get_lora_bert_embeddings(X_train, model, tokenizer)\n",
    "X_test_emb = get_lora_bert_embeddings(X_test, model, tokenizer)\n",
    "\n",
    "# 2. Fit ridge regression for each voxel\n",
    "ridge = Ridge(alpha=1.0)\n",
    "ridge.fit(X_train_emb, Y_train[:, voxel_index])\n",
    "y_pred = ridge.predict(X_test_emb)\n",
    "\n",
    "# 3. Evaluate R²\n",
    "r2_ridge_lora = r2_score(Y_test[:, voxel_index], y_pred)\n",
    "print(f\"Ridge R² using LoRA-embedding for voxel {voxel_index}: {r2_ridge_lora:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c7ff7c",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a561b31-b4e0-48bc-90df-1bbc098597a8",
   "metadata": {},
   "source": [
    "# 1) Identifying voxels where the model performs well"
   ]
  },
  {
   "cell_type": "raw",
   "id": "80fb0b67-39ed-41de-ae69-270dbd902e5c",
   "metadata": {},
   "source": [
    "Required work from part 1: \n",
    "\n",
    "Once final encoder model has been trained (LORA?), then we use that encoder model to create embeddings and run ridge regression on only a single story. Then we want to save those correlation coefficient results to our folder as ridge_corr_subject2.npy. Then once that is commplete, the following code should run fine!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae7a4054",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'ridge_corr_subject2.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Load the fine-tuned encoder\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m ccs \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mridge_corr_subject2.npy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# shape: (num_alphas, num_voxels)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m best_cc \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(ccs, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# shape: (num_voxels,)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Pick top-performing voxels (e.g., top 5%)\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/env_214/lib/python3.13/site-packages/numpy/lib/_npyio_impl.py:451\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    449\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 451\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    452\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ridge_corr_subject2.npy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Load the fine-tuned encoder\n",
    "ccs = np.load(\"ridge_corr_subject2.npy\")  # shape: (num_alphas, num_voxels)\n",
    "best_cc = np.max(ccs, axis=0)  # shape: (num_voxels,)\n",
    "\n",
    "# Pick top-performing voxels (e.g., top 5%)\n",
    "top_n = int(0.05 * len(best_cc))\n",
    "top_voxel_indices = np.argsort(best_cc)[-top_n:]\n",
    "print(f\"Top {top_n} voxel indices selected for interpretation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db83bc8-5a1a-4fdf-a5e0-5563102c0530",
   "metadata": {},
   "source": [
    "# Running SHAP and LIME to Identigy Influential Words"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7539a8eb-18b4-4daf-a4a1-64a27f2ba248",
   "metadata": {},
   "source": [
    "Required work:\n",
    "\n",
    "\"ridge_weights_top_voxels.npy\" needs to exist and must be generated after we find the top voxels in the part above.\n",
    "\n",
    "need something like the following in the code above, but will add once the code is complete.\n",
    "\n",
    "ridge_weights = coefs[top_voxel_indices, :]  # shape: (top_voxels, embedding_dim)\n",
    "np.save(\"ridge_weights_top_voxels.npy\", ridge_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de9cc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from encoder import Encoder \n",
    "from train_encoder import train_bert\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load tokenizer and fine-tuned encoder\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "encoder = Encoder(vocab_size=tokenizer.vocab_size)\n",
    "encoder.load_state_dict(torch.load(\"finetuned_encoder.pt\"))\n",
    "encoder.to(device).eval()\n",
    "\n",
    "# Load raw text + extract one test story\n",
    "with open(\"/ocean/projects/mth240012p/shared/data/raw_text.pkl\", \"rb\") as f:\n",
    "    raw_text = pickle.load(f)\n",
    "\n",
    "story_name = \"losing_my_legs\"\n",
    "sentences = [\" \".join(w) for w in raw_text[story_name].data if len(w) > 0][:50]  # Trimmed for speed!!\n",
    "\n",
    "# Return voxel predictions from BERT CLS token\n",
    "def predict_voxel_activation(texts):\n",
    "    inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "    input_ids = inputs['input_ids'].to(device)\n",
    "    token_type_ids = inputs['token_type_ids'].to(device)\n",
    "    attention_mask = inputs['attention_mask'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        hidden = encoder(input_ids, token_type_ids, attention_mask)\n",
    "        cls_embeds = hidden[:, 0, :]  # CLS token\n",
    "\n",
    "    # Use pretrained ridge weights per voxel (top_voxel_indices)\n",
    "    ridge_weights = np.load(\"ridge_weights_top_voxels.npy\")  # (voxel_count, cls_dim)\n",
    "    preds = cls_embeds.cpu().numpy() @ ridge_weights.T        # (num_sentences, top_voxels)\n",
    "    return preds\n",
    "\n",
    "# SHAP \n",
    "shap_explainer = shap.Explainer(predict_voxel_activation, tokenizer)\n",
    "shap_values = shap_explainer(sentences)\n",
    "\n",
    "# LIME\n",
    "lime_explainer = LimeTextExplainer(class_names=[f\"Voxel {i}\" for i in top_voxel_indices])\n",
    "lime_exp = lime_explainer.explain_instance(sentences[0], predict_voxel_activation, num_features=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b19f8ee-561e-42e2-93d0-f9018cb3d1df",
   "metadata": {},
   "source": [
    "# Comparing SHAP and LIME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b555706f-2643-4974-a9f3-1909c781b404",
   "metadata": {},
   "source": [
    "Required work:\n",
    "\n",
    "We will add side-by-side comparisons and try across a couple more voxels. could have something like the following:\n",
    "\n",
    "EXAMPLE CODE\n",
    "<!-- \n",
    "def compare_word_importance(shap_values, lime_exp, voxel_idx):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    # SHAP bar plot\n",
    "    shap.plots.bar(shap_values[:, :, voxel_idx].mean(0), show=False, ax=axes[0])\n",
    "    axes[0].set_title(f\"SHAP - Voxel {voxel_idx}\")\n",
    "    \n",
    "    # LIME bar plot\n",
    "    lime_exp.as_pyplot_figure(axes[1])\n",
    "    axes[1].set_title(f\"LIME - Voxel {voxel_idx}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "compare_word_importance(shap_values, lime_exp, voxel_idx=0)\n",
    " -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404ce714-21d5-4851-ad88-681bc5b2b7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# SHAP plot\n",
    "shap.plots.text(shap_values[0]) \n",
    "shap.plots.bar(shap_values[:, :, 0].mean(0))  # average over sentences for Voxel 0\n",
    "\n",
    "# LIME plot\n",
    "lime_fig = lime_exp.as_pyplot_figure()\n",
    "plt.title(\"LIME Word Importance for One Sentence (Voxel-Level)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed3b42c-2212-4a0c-a5b2-0cff9e0790e7",
   "metadata": {},
   "source": [
    "# Repeat all analysis for another test-story"
   ]
  },
  {
   "cell_type": "raw",
   "id": "21884b3a-0045-4ae6-a3d2-6f934041caad",
   "metadata": {},
   "source": [
    "Required work:\n",
    "\n",
    "just repeat all code shown above but for a new story and compare the word importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352ea46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just change the story name and re-run Section 2 onward??? not sure\n",
    "story_name = \"myfirstdaywiththeyankees\"\n",
    "sentences = [\" \".join(w) for w in raw_text[story_name].data if len(w) > 0][:50]\n",
    "\n",
    "# Rerun: predict_voxel_activation(), SHAP, and LIME\n",
    "shap_values = shap_explainer(sentences)\n",
    "shap.plots.text(shap_values[0])\n",
    "lime_exp = lime_explainer.explain_instance(sentences[0], predict_voxel_activation, num_features=10)\n",
    "lime_fig = lime_exp.as_pyplot_figure()\n",
    "plt.title(\"LIME Word Importance for Another Sentence (Voxel-Level)\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
