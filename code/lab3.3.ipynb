{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07d1799c-84ba-4bf8-86ef-4abd9d9c2424",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3859fffc-5883-438a-b64f-dea9c9dbfbf1",
   "metadata": {},
   "source": [
    "### Ridge Regression\n",
    "In this step, we use a pre-trained BERT model (bert-base-uncased) to extract CLS token embeddings for each TR (timepoint) in every story. Each TR contains a list of words, which are tokenized and encoded using BERT. These embeddings are then aligned with their corresponding fMRI responses. We apply z-score normalization and temporal delays to the BERT embeddings and fit a ridge regression model to predict voxel-level brain activity from the BERT representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc0fc577-d2d8-4c78-849f-bc1b6f2a0f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw_text.pkl and fMRI...\n",
      "Valid stories with fMRI: 101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing stories: 100%|██████████| 101/101 [38:43<00:00, 23.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collected 34700 TRs | BERT dim: 768 | Voxels: 94251\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ridge_utils.utils import zscore, make_delayed\n",
    "from ridge_utils.ridge import ridge_corr\n",
    "\n",
    "# -------------------- Step 1: Loading -------------------- #\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "data_dir = \"/ocean/projects/mth240012p/shared/data\"\n",
    "subject = \"subject2\"\n",
    "max_tokens = 50\n",
    "\n",
    "print(\"Loading raw_text.pkl and fMRI...\")\n",
    "with open(os.path.join(data_dir, \"raw_text.pkl\"), \"rb\") as f:\n",
    "    raw_texts = pickle.load(f)\n",
    "\n",
    "story_names = []\n",
    "Y_dict = {}\n",
    "for story in raw_texts:\n",
    "    fmri_path = os.path.join(data_dir, subject, f\"{story}.npy\")\n",
    "    if os.path.exists(fmri_path):\n",
    "        Y_dict[story] = np.load(fmri_path)\n",
    "        story_names.append(story)\n",
    "\n",
    "print(f\"Valid stories with fMRI: {len(story_names)}\")\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\") # loading BERT Model\n",
    "bert_model = BertModel.from_pretrained(\"bert-base-uncased\").to(device)\n",
    "bert_model.eval()\n",
    "\n",
    "# -------------------- Step 2: Iterate over each story and each TR -------------------- #\n",
    "X_all, Y_all = [], []\n",
    "\n",
    "for story in tqdm(story_names, desc=\"Processing stories\"):\n",
    "    fmri = Y_dict[story]\n",
    "    ds = raw_texts[story] \n",
    "    for i in range(len(ds.data)):\n",
    "        word_list = ds.data[i] \n",
    "        if not word_list or i >= fmri.shape[0]:\n",
    "            continue  \n",
    "        sentence = \" \".join(word_list)\n",
    "        inputs = tokenizer(\n",
    "            sentence,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"max_length\",\n",
    "            max_length=512,\n",
    "            truncation=True\n",
    "        )\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = bert_model(**inputs)\n",
    "        cls_embed = outputs.last_hidden_state[:, 0, :].squeeze().cpu().numpy()\n",
    "        X_all.append(cls_embed)\n",
    "        Y_all.append(fmri[i])\n",
    "\n",
    "X_all = np.array(X_all)\n",
    "Y_all = np.array(Y_all)\n",
    "\n",
    "print(f\"\\nCollected {X_all.shape[0]} TRs | BERT dim: {X_all.shape[1]} | Voxels: {Y_all.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "186a78b1-142a-4176-9616-a678fc9698c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running ridge regression...\n",
      "Final shapes → X: (34696, 3840), Y: (34696, 94251)\n",
      "Train: (27756, 3840), Test: (6940, 3840)\n",
      "Ridge correlation shape: (20, 94251)\n",
      "Mean CC:    0.0067\n",
      "Median CC:  0.0065\n",
      "Top 1% CC:  0.0383\n",
      "Top 5% CC:  0.0306\n"
     ]
    }
   ],
   "source": [
    "X_z = zscore(X_all.T).T\n",
    "Y_z = zscore(Y_all)\n",
    "Y_z = np.nan_to_num(Y_z)\n",
    "\n",
    "X_delayed = make_delayed(X_z[4:], delays=[0, 1, 2, 3, 4])\n",
    "Y_trimmed = Y_z[4:]\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X_delayed, Y_trimmed, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# -------------------- Step 3: Ridge Regression -------------------- #\n",
    "print(\"Running ridge regression...\")\n",
    "\n",
    "print(f\"Final shapes → X: {X_delayed.shape}, Y: {Y_trimmed.shape}\")\n",
    "print(f\"Train: {X_train.shape}, Test: {X_test.shape}\")\n",
    "\n",
    "alphas = np.logspace(1, 3, 20)\n",
    "ccs = np.array(ridge_corr(X_train, X_test, Y_train, Y_test, alphas))\n",
    "\n",
    "print(\"Ridge correlation shape:\", ccs.shape)\n",
    "best_cc = np.max(ccs, axis=0)\n",
    "print(f\"Mean CC:    {np.mean(best_cc):.4f}\")\n",
    "print(f\"Median CC:  {np.median(best_cc):.4f}\")\n",
    "print(f\"Top 1% CC:  {np.mean(np.sort(best_cc)[-int(0.01*len(best_cc)):]):.4f}\")\n",
    "print(f\"Top 5% CC:  {np.mean(np.sort(best_cc)[-int(0.05*len(best_cc)):]):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "73440276-29d6-4253-90f1-eed66afd4d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_delayed shape: (34696, 3840)\n",
      "Y_trimmed shape: (34696, 94251)\n",
      "NaN in X: False | Inf in X: False\n",
      "NaN in Y: False | Inf in Y: False\n",
      "X mean/std: 1.2600142719097444e-07 0.9999527895398399\n",
      "Y mean/std: 8.215887372834887e-20 0.9887539174267306\n",
      "All zeros in Y: False\n",
      "Max NaN count per voxel: 776 / 34700\n",
      "Number of voxels with any NaN: 32\n"
     ]
    }
   ],
   "source": [
    "# Check the Result \n",
    "print(\"X_delayed shape:\", X_delayed.shape)\n",
    "print(\"Y_trimmed shape:\", Y_trimmed.shape)\n",
    "print(\"NaN in X:\", np.isnan(X_delayed).any(), \"| Inf in X:\", np.isinf(X_delayed).any())\n",
    "print(\"NaN in Y:\", np.isnan(Y_trimmed).any(), \"| Inf in Y:\", np.isinf(Y_trimmed).any())\n",
    "print(\"X mean/std:\", np.mean(X_delayed), np.std(X_delayed))\n",
    "print(\"Y mean/std:\", np.mean(Y_trimmed), np.std(Y_trimmed))\n",
    "print(\"All zeros in Y:\", np.all(Y_trimmed == 0))\n",
    "nan_voxels = np.isnan(Y_all).sum(axis=0)\n",
    "print(f\"Max NaN count per voxel: {nan_voxels.max()} / {Y_all.shape[0]}\")\n",
    "print(f\"Number of voxels with any NaN: {np.sum(nan_voxels > 0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5010a9-40e4-4f29-8c34-eecca0814aef",
   "metadata": {},
   "source": [
    "### Fine-Tuning BERT Encoder on Stimulus Texts\n",
    "\n",
    "In this section, we fine-tune the BERT encoder using the raw story text data via a Masked Language Modeling (MLM) objective.\n",
    "\n",
    "We use the following approach:\n",
    "\n",
    "- **Dataset Construction**: We convert each story's TR-aligned text (from the `DataSequence`) into input IDs for BERT, masking a subset of tokens randomly to form an MLM task. Each TR is treated as one training example.\n",
    "- **Model Architecture**: We reuse the `Encoder` defined in `encoder.py`, which wraps a pre-trained BERT model (`bert-base-uncased`) with a linear decoder head to predict masked tokens.\n",
    "- **Training Loop**: We leverage `train_encoder.py` to fine-tune the model over multiple epochs. The training loss is computed only on masked positions.\n",
    "\n",
    "This fine-tuning step helps the BERT encoder adapt to the linguistic distribution of the experimental stimuli, improving the quality of the learned embeddings for downstream voxel prediction.\n",
    "\n",
    "After training, we will save the encoder weights, which can later be loaded and used for voxel-wise ridge regression in Part 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2c886e40-a09e-41b3-b700-1346563f108b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataset for fine-tuning...\n",
      "Initializing encoder model...\n",
      "Starting MLM training...\n",
      "Epoch [1/3], Train Loss: 1.0677, Val Loss: nan\n",
      "Epoch [2/3], Train Loss: 0.7351, Val Loss: 0.6887\n",
      "Epoch [3/3], Train Loss: 0.6588, Val Loss: nan\n",
      "Fine-tuned encoder saved as 'finetuned_encoder.pt'\n"
     ]
    }
   ],
   "source": [
    "# ======================= Fine-tuning Setup ============================\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from train_encoder import train_bert\n",
    "from encoder import Encoder\n",
    "\n",
    "# 1. Prepare Dataset for MLM pretraining\n",
    "class MLMTextDataset(Dataset):\n",
    "    def __init__(self, raw_texts, tokenizer, max_length=128):\n",
    "        self.samples = []\n",
    "        for story in raw_texts.values():\n",
    "            for word_list in story.data:\n",
    "                if not word_list:\n",
    "                    continue\n",
    "                sentence = \" \".join(word_list)\n",
    "                tokens = tokenizer(\n",
    "                    sentence,\n",
    "                    max_length=max_length,\n",
    "                    truncation=True,\n",
    "                    padding='max_length',\n",
    "                    return_tensors='pt'\n",
    "                )\n",
    "                self.samples.append(tokens)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.samples[idx]\n",
    "        return {k: v.squeeze(0) for k, v in item.items()}\n",
    "\n",
    "# 2. Initialize model, tokenizer, and dataset\n",
    "print(\"Preparing dataset for fine-tuning...\")\n",
    "dataset = MLMTextDataset(raw_texts, tokenizer, max_length=128)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "print(\"Initializing encoder model...\")\n",
    "encoder = Encoder(vocab_size=tokenizer.vocab_size).to(device)\n",
    "\n",
    "# 3. Train the encoder using MLM objective\n",
    "print(\"Starting MLM training...\")\n",
    "train_bert(\n",
    "    model=encoder,\n",
    "    dataloader=dataloader,\n",
    "    tokenizer=tokenizer,\n",
    "    device=device,\n",
    "    epochs=3, # Change this if you want\n",
    "    lr=1e-4 # Can change into smaller\n",
    ")\n",
    "\n",
    "# 4. Save encoder for later use\n",
    "torch.save(encoder.state_dict(), \"finetuned_encoder.pt\") # use this model for new BERT Model\n",
    "print(\"Fine-tuned encoder saved as 'finetuned_encoder.pt'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c7ff7c",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a561b31-b4e0-48bc-90df-1bbc098597a8",
   "metadata": {},
   "source": [
    "# 1) Identifying voxels where the model performs well"
   ]
  },
  {
   "cell_type": "raw",
   "id": "80fb0b67-39ed-41de-ae69-270dbd902e5c",
   "metadata": {},
   "source": [
    "Required work from part 1: \n",
    "\n",
    "Once final encoder model has been trained (LORA?), then we use that encoder model to create embeddings and run ridge regression on only a single story. Then we want to save those correlation coefficient results to our folder as ridge_corr_subject2.npy. Then once that is commplete, the following code should run fine!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae7a4054",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'ridge_corr_subject2.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Load the fine-tuned encoder\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m ccs \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mridge_corr_subject2.npy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# shape: (num_alphas, num_voxels)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m best_cc \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(ccs, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# shape: (num_voxels,)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Pick top-performing voxels (e.g., top 5%)\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/env_214/lib/python3.13/site-packages/numpy/lib/_npyio_impl.py:451\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    449\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 451\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    452\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ridge_corr_subject2.npy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Load the fine-tuned encoder\n",
    "ccs = np.load(\"ridge_corr_subject2.npy\")  # shape: (num_alphas, num_voxels)\n",
    "best_cc = np.max(ccs, axis=0)  # shape: (num_voxels,)\n",
    "\n",
    "# Pick top-performing voxels (e.g., top 5%)\n",
    "top_n = int(0.05 * len(best_cc))\n",
    "top_voxel_indices = np.argsort(best_cc)[-top_n:]\n",
    "print(f\"Top {top_n} voxel indices selected for interpretation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db83bc8-5a1a-4fdf-a5e0-5563102c0530",
   "metadata": {},
   "source": [
    "# Running SHAP and LIME to Identigy Influential Words"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7539a8eb-18b4-4daf-a4a1-64a27f2ba248",
   "metadata": {},
   "source": [
    "Required work:\n",
    "\n",
    "\"ridge_weights_top_voxels.npy\" needs to exist and must be generated after we find the top voxels in the part above.\n",
    "\n",
    "need something like the following in the code above, but will add once the code is complete.\n",
    "\n",
    "ridge_weights = coefs[top_voxel_indices, :]  # shape: (top_voxels, embedding_dim)\n",
    "np.save(\"ridge_weights_top_voxels.npy\", ridge_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de9cc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from encoder import Encoder \n",
    "from train_encoder import train_bert\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load tokenizer and fine-tuned encoder\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "encoder = Encoder(vocab_size=tokenizer.vocab_size)\n",
    "encoder.load_state_dict(torch.load(\"finetuned_encoder.pt\"))\n",
    "encoder.to(device).eval()\n",
    "\n",
    "# Load raw text + extract one test story\n",
    "with open(\"/ocean/projects/mth240012p/shared/data/raw_text.pkl\", \"rb\") as f:\n",
    "    raw_text = pickle.load(f)\n",
    "\n",
    "story_name = \"losing_my_legs\"\n",
    "sentences = [\" \".join(w) for w in raw_text[story_name].data if len(w) > 0][:50]  # Trimmed for speed!!\n",
    "\n",
    "# Return voxel predictions from BERT CLS token\n",
    "def predict_voxel_activation(texts):\n",
    "    inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "    input_ids = inputs['input_ids'].to(device)\n",
    "    token_type_ids = inputs['token_type_ids'].to(device)\n",
    "    attention_mask = inputs['attention_mask'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        hidden = encoder(input_ids, token_type_ids, attention_mask)\n",
    "        cls_embeds = hidden[:, 0, :]  # CLS token\n",
    "\n",
    "    # Use pretrained ridge weights per voxel (top_voxel_indices)\n",
    "    ridge_weights = np.load(\"ridge_weights_top_voxels.npy\")  # (voxel_count, cls_dim)\n",
    "    preds = cls_embeds.cpu().numpy() @ ridge_weights.T        # (num_sentences, top_voxels)\n",
    "    return preds\n",
    "\n",
    "# SHAP \n",
    "shap_explainer = shap.Explainer(predict_voxel_activation, tokenizer)\n",
    "shap_values = shap_explainer(sentences)\n",
    "\n",
    "# LIME\n",
    "lime_explainer = LimeTextExplainer(class_names=[f\"Voxel {i}\" for i in top_voxel_indices])\n",
    "lime_exp = lime_explainer.explain_instance(sentences[0], predict_voxel_activation, num_features=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b19f8ee-561e-42e2-93d0-f9018cb3d1df",
   "metadata": {},
   "source": [
    "# Comparing SHAP and LIME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b555706f-2643-4974-a9f3-1909c781b404",
   "metadata": {},
   "source": [
    "Required work:\n",
    "\n",
    "We will add side-by-side comparisons and try across a couple more voxels. could have something like the following:\n",
    "\n",
    "EXAMPLE CODE\n",
    "<!-- \n",
    "def compare_word_importance(shap_values, lime_exp, voxel_idx):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    # SHAP bar plot\n",
    "    shap.plots.bar(shap_values[:, :, voxel_idx].mean(0), show=False, ax=axes[0])\n",
    "    axes[0].set_title(f\"SHAP - Voxel {voxel_idx}\")\n",
    "    \n",
    "    # LIME bar plot\n",
    "    lime_exp.as_pyplot_figure(axes[1])\n",
    "    axes[1].set_title(f\"LIME - Voxel {voxel_idx}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "compare_word_importance(shap_values, lime_exp, voxel_idx=0)\n",
    " -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404ce714-21d5-4851-ad88-681bc5b2b7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# SHAP plot\n",
    "shap.plots.text(shap_values[0]) \n",
    "shap.plots.bar(shap_values[:, :, 0].mean(0))  # average over sentences for Voxel 0\n",
    "\n",
    "# LIME plot\n",
    "lime_fig = lime_exp.as_pyplot_figure()\n",
    "plt.title(\"LIME Word Importance for One Sentence (Voxel-Level)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed3b42c-2212-4a0c-a5b2-0cff9e0790e7",
   "metadata": {},
   "source": [
    "# Repeat all analysis for another test-story"
   ]
  },
  {
   "cell_type": "raw",
   "id": "21884b3a-0045-4ae6-a3d2-6f934041caad",
   "metadata": {},
   "source": [
    "Required work:\n",
    "\n",
    "just repeat all code shown above but for a new story and compare the word importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352ea46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just change the story name and re-run Section 2 onward??? not sure\n",
    "story_name = \"myfirstdaywiththeyankees\"\n",
    "sentences = [\" \".join(w) for w in raw_text[story_name].data if len(w) > 0][:50]\n",
    "\n",
    "# Rerun: predict_voxel_activation(), SHAP, and LIME\n",
    "shap_values = shap_explainer(sentences)\n",
    "shap.plots.text(shap_values[0])\n",
    "lime_exp = lime_explainer.explain_instance(sentences[0], predict_voxel_activation, num_features=10)\n",
    "lime_fig = lime_exp.as_pyplot_figure()\n",
    "plt.title(\"LIME Word Importance for Another Sentence (Voxel-Level)\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_214",
   "language": "python",
   "name": "env_214"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
