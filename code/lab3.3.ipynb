{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07d1799c-84ba-4bf8-86ef-4abd9d9c2424",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3859fffc-5883-438a-b64f-dea9c9dbfbf1",
   "metadata": {},
   "source": [
    "### Ridge Regression\n",
    "In this step, we use a pre-trained BERT model (bert-base-uncased) to extract CLS token embeddings for each TR (timepoint) in every story. Each TR contains a list of words, which are tokenized and encoded using BERT. These embeddings are then aligned with their corresponding fMRI responses. We apply z-score normalization and temporal delays to the BERT embeddings and fit a ridge regression model to predict voxel-level brain activity from the BERT representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc0fc577-d2d8-4c78-849f-bc1b6f2a0f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw_text.pkl and fMRI...\n",
      "Valid stories with fMRI: 101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing stories: 100%|██████████| 101/101 [38:43<00:00, 23.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collected 34700 TRs | BERT dim: 768 | Voxels: 94251\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ridge_utils.utils import zscore, make_delayed\n",
    "from ridge_utils.ridge import ridge_corr\n",
    "\n",
    "# -------------------- Step 1: Loading -------------------- #\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "data_dir = \"/ocean/projects/mth240012p/shared/data\"\n",
    "subject = \"subject2\"\n",
    "max_tokens = 50\n",
    "\n",
    "print(\"Loading raw_text.pkl and fMRI...\")\n",
    "with open(os.path.join(data_dir, \"raw_text.pkl\"), \"rb\") as f:\n",
    "    raw_texts = pickle.load(f)\n",
    "\n",
    "story_names = []\n",
    "Y_dict = {}\n",
    "for story in raw_texts:\n",
    "    fmri_path = os.path.join(data_dir, subject, f\"{story}.npy\")\n",
    "    if os.path.exists(fmri_path):\n",
    "        Y_dict[story] = np.load(fmri_path)\n",
    "        story_names.append(story)\n",
    "\n",
    "print(f\"Valid stories with fMRI: {len(story_names)}\")\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\") # loading BERT Model\n",
    "bert_model = BertModel.from_pretrained(\"bert-base-uncased\").to(device)\n",
    "bert_model.eval()\n",
    "\n",
    "# -------------------- Step 2: Iterate over each story and each TR -------------------- #\n",
    "X_all, Y_all = [], []\n",
    "\n",
    "for story in tqdm(story_names, desc=\"Processing stories\"):\n",
    "    fmri = Y_dict[story]\n",
    "    ds = raw_texts[story] \n",
    "    for i in range(len(ds.data)):\n",
    "        word_list = ds.data[i] \n",
    "        if not word_list or i >= fmri.shape[0]:\n",
    "            continue  \n",
    "        sentence = \" \".join(word_list)\n",
    "        inputs = tokenizer(\n",
    "            sentence,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"max_length\",\n",
    "            max_length=512,\n",
    "            truncation=True\n",
    "        )\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = bert_model(**inputs)\n",
    "        cls_embed = outputs.last_hidden_state[:, 0, :].squeeze().cpu().numpy()\n",
    "        X_all.append(cls_embed)\n",
    "        Y_all.append(fmri[i])\n",
    "\n",
    "X_all = np.array(X_all)\n",
    "Y_all = np.array(Y_all)\n",
    "\n",
    "print(f\"\\nCollected {X_all.shape[0]} TRs | BERT dim: {X_all.shape[1]} | Voxels: {Y_all.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "186a78b1-142a-4176-9616-a678fc9698c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running ridge regression...\n",
      "Final shapes → X: (34696, 3840), Y: (34696, 94251)\n",
      "Train: (27756, 3840), Test: (6940, 3840)\n",
      "Ridge correlation shape: (20, 94251)\n",
      "Mean CC:    0.0067\n",
      "Median CC:  0.0065\n",
      "Top 1% CC:  0.0383\n",
      "Top 5% CC:  0.0306\n"
     ]
    }
   ],
   "source": [
    "X_z = zscore(X_all.T).T\n",
    "Y_z = zscore(Y_all)\n",
    "Y_z = np.nan_to_num(Y_z)\n",
    "\n",
    "X_delayed = make_delayed(X_z[4:], delays=[0, 1, 2, 3, 4])\n",
    "Y_trimmed = Y_z[4:]\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X_delayed, Y_trimmed, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# -------------------- Step 3: Ridge Regression -------------------- #\n",
    "print(\"Running ridge regression...\")\n",
    "\n",
    "print(f\"Final shapes → X: {X_delayed.shape}, Y: {Y_trimmed.shape}\")\n",
    "print(f\"Train: {X_train.shape}, Test: {X_test.shape}\")\n",
    "\n",
    "alphas = np.logspace(1, 3, 20)\n",
    "ccs = np.array(ridge_corr(X_train, X_test, Y_train, Y_test, alphas))\n",
    "\n",
    "print(\"Ridge correlation shape:\", ccs.shape)\n",
    "best_cc = np.max(ccs, axis=0)\n",
    "print(f\"Mean CC:    {np.mean(best_cc):.4f}\")\n",
    "print(f\"Median CC:  {np.median(best_cc):.4f}\")\n",
    "print(f\"Top 1% CC:  {np.mean(np.sort(best_cc)[-int(0.01*len(best_cc)):]):.4f}\")\n",
    "print(f\"Top 5% CC:  {np.mean(np.sort(best_cc)[-int(0.05*len(best_cc)):]):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "73440276-29d6-4253-90f1-eed66afd4d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_delayed shape: (34696, 3840)\n",
      "Y_trimmed shape: (34696, 94251)\n",
      "NaN in X: False | Inf in X: False\n",
      "NaN in Y: False | Inf in Y: False\n",
      "X mean/std: 1.2600142719097444e-07 0.9999527895398399\n",
      "Y mean/std: 8.215887372834887e-20 0.9887539174267306\n",
      "All zeros in Y: False\n",
      "Max NaN count per voxel: 776 / 34700\n",
      "Number of voxels with any NaN: 32\n"
     ]
    }
   ],
   "source": [
    "# Check the Result \n",
    "print(\"X_delayed shape:\", X_delayed.shape)\n",
    "print(\"Y_trimmed shape:\", Y_trimmed.shape)\n",
    "print(\"NaN in X:\", np.isnan(X_delayed).any(), \"| Inf in X:\", np.isinf(X_delayed).any())\n",
    "print(\"NaN in Y:\", np.isnan(Y_trimmed).any(), \"| Inf in Y:\", np.isinf(Y_trimmed).any())\n",
    "print(\"X mean/std:\", np.mean(X_delayed), np.std(X_delayed))\n",
    "print(\"Y mean/std:\", np.mean(Y_trimmed), np.std(Y_trimmed))\n",
    "print(\"All zeros in Y:\", np.all(Y_trimmed == 0))\n",
    "nan_voxels = np.isnan(Y_all).sum(axis=0)\n",
    "print(f\"Max NaN count per voxel: {nan_voxels.max()} / {Y_all.shape[0]}\")\n",
    "print(f\"Number of voxels with any NaN: {np.sum(nan_voxels > 0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5010a9-40e4-4f29-8c34-eecca0814aef",
   "metadata": {},
   "source": [
    "### Fine-Tuning BERT Encoder on Stimulus Texts\n",
    "\n",
    "In this section, we fine-tune the BERT encoder using the raw story text data via a Masked Language Modeling (MLM) objective.\n",
    "\n",
    "We use the following approach:\n",
    "\n",
    "- **Dataset Construction**: We convert each story's TR-aligned text (from the `DataSequence`) into input IDs for BERT, masking a subset of tokens randomly to form an MLM task. Each TR is treated as one training example.\n",
    "- **Model Architecture**: We reuse the `Encoder` defined in `encoder.py`, which wraps a pre-trained BERT model (`bert-base-uncased`) with a linear decoder head to predict masked tokens.\n",
    "- **Training Loop**: We leverage `train_encoder.py` to fine-tune the model over multiple epochs. The training loss is computed only on masked positions.\n",
    "\n",
    "This fine-tuning step helps the BERT encoder adapt to the linguistic distribution of the experimental stimuli, improving the quality of the learned embeddings for downstream voxel prediction.\n",
    "\n",
    "After training, we will save the encoder weights, which can later be loaded and used for voxel-wise ridge regression in Part 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2c886e40-a09e-41b3-b700-1346563f108b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataset for fine-tuning...\n",
      "Initializing encoder model...\n",
      "Starting MLM training...\n",
      "Epoch [1/3], Train Loss: 1.0677, Val Loss: nan\n",
      "Epoch [2/3], Train Loss: 0.7351, Val Loss: 0.6887\n",
      "Epoch [3/3], Train Loss: 0.6588, Val Loss: nan\n",
      "Fine-tuned encoder saved as 'finetuned_encoder.pt'\n"
     ]
    }
   ],
   "source": [
    "# ======================= Fine-tuning Setup ============================\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from train_encoder import train_bert\n",
    "from encoder import Encoder\n",
    "\n",
    "# 1. Prepare Dataset for MLM pretraining\n",
    "class MLMTextDataset(Dataset):\n",
    "    def __init__(self, raw_texts, tokenizer, max_length=128):\n",
    "        self.samples = []\n",
    "        for story in raw_texts.values():\n",
    "            for word_list in story.data:\n",
    "                if not word_list:\n",
    "                    continue\n",
    "                sentence = \" \".join(word_list)\n",
    "                tokens = tokenizer(\n",
    "                    sentence,\n",
    "                    max_length=max_length,\n",
    "                    truncation=True,\n",
    "                    padding='max_length',\n",
    "                    return_tensors='pt'\n",
    "                )\n",
    "                self.samples.append(tokens)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.samples[idx]\n",
    "        return {k: v.squeeze(0) for k, v in item.items()}\n",
    "\n",
    "# 2. Initialize model, tokenizer, and dataset\n",
    "print(\"Preparing dataset for fine-tuning...\")\n",
    "dataset = MLMTextDataset(raw_texts, tokenizer, max_length=128)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "print(\"Initializing encoder model...\")\n",
    "encoder = Encoder(vocab_size=tokenizer.vocab_size).to(device)\n",
    "\n",
    "# 3. Train the encoder using MLM objective\n",
    "print(\"Starting MLM training...\")\n",
    "train_bert(\n",
    "    model=encoder,\n",
    "    dataloader=dataloader,\n",
    "    tokenizer=tokenizer,\n",
    "    device=device,\n",
    "    epochs=3, # Change this if you want\n",
    "    lr=1e-4 # Can change into smaller\n",
    ")\n",
    "\n",
    "# 4. Save encoder for later use\n",
    "torch.save(encoder.state_dict(), \"finetuned_encoder.pt\") # use this model for new BERT Model\n",
    "print(\"Fine-tuned encoder saved as 'finetuned_encoder.pt'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_214",
   "language": "python",
   "name": "env_214"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
